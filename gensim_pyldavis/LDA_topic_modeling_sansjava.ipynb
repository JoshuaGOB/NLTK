{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Unsupervised learning: Latent Dirichlet allocation (LDA) topic modeling*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lda in /Users/anaconda3/envs/jupyter_usage/lib/python3.6/site-packages (1.1.0)\n",
      "Requirement already satisfied: pbr<4,>=0.6 in /Users/anaconda3/envs/jupyter_usage/lib/python3.6/site-packages (from lda) (3.1.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.13.0 in /Users/anaconda3/envs/jupyter_usage/lib/python3.6/site-packages (from lda) (1.15.1)\n",
      "Requirement already satisfied: nltk in /Users/anaconda3/envs/jupyter_usage/lib/python3.6/site-packages (3.3)\n",
      "Requirement already satisfied: six in /Users/anaconda3/envs/jupyter_usage/lib/python3.6/site-packages (from nltk) (1.11.0)\n",
      "Collecting scikit-learn\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/db/f6375ee4b604209d88447bffab074f236d5357a4f6fa38901362311ed18d/scikit_learn-0.19.2-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (7.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 7.1MB 4.4MB/s eta 0:00:011\n",
      "\u001b[?25hInstalling collected packages: scikit-learn\n",
      "Successfully installed scikit-learn-0.19.2\n"
     ]
    }
   ],
   "source": [
    "## Install Python package for LDA\n",
    "# http://pythonhosted.org/lda/getting_started.html\n",
    "\n",
    "!pip install lda\n",
    "!pip install nltk\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing basic packages\n",
    "\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Volumes/benson/Mellon/cirma_related/Topic_Modeling_Workshop_Materials/cirma_texts/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_list = []\n",
    "\n",
    "for filename in [item for item in os.listdir('./') if '.txt' in item]:\n",
    "    text_data = open(filename).read()\n",
    "    document_list.append(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Importing NLTK stop words\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    " \n",
    "stop_words = stopwords.words('spanish') + [\"'s\", \"'re\", '”', '“', '’', '—'] + list(string.punctuation)\n",
    "\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenizing and removing stop words from our list of documents\n",
    "\n",
    "documents_filtered = []\n",
    "\n",
    "for document in document_list:\n",
    "    token_list = word_tokenize(document.lower())\n",
    "    tokens_filtered = [item for item in token_list if (item not in stop_words)]\n",
    "    documents_filtered.append(' '.join(tokens_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"distintos lugares pais localizan ocho cadaveres baleados vejados cuatro víctimas podido ser identificadas gabinete identificación policía ocho cadáveres perforaciones bala vejámenes continuando carnicería humana sufre país algún tiempo localizados distintas partes siendo identificadas únicamente cuatro víctimas camino terracería aldea san miguel pajapa municipio pajapita conduce cantón ixcahuin municipio nuevo progreso san marcos localizado cadáver perforado tiros víctima supuestamente secuestrada identificada angel paulo orozco lópez 33 años edad siendo trasladada anfiteatro coatepeque parte carretera ciudad escuintla conduce colonia portales encontrado asesinado dos balazos tórax rostro señor andrés lópez 42 años edad autores crimen utilizaron armas calibre 38 milímetros informó mañana policía nacional realiza investigaciones terrenos finca agua caliente —propiedad señor porfirio orellana— ubicada jurisdicción monjas departamento jalapa descubierto cadáver varias perforaciones bala autoridades policíacas juez paz constituyeron lugar hallazgo víctima además perforaciones bala señales tortura reconocida rené aguerrido 28 años edad dos desconocidos finca florencia mientras terrenos finca comunal florencia '' situada jurisdicción santa lucía milpas altas 15 metros carretera hacia antigua guatemala descubiertos dos cadáveres numerosas perforaciones bala señales vejámenes víctimas 28 30 años edad respectivamente identificadas según policía nacional autigua guatemala crímenes cometidos posiblemente hace tres días pues descomposición aldea santa rosa municipio amates departamento izabal asesinado balazos joven ambrosio morales 26 años edad presunto autor crimen identificado gregorio espino sido capturado finalmente municipio san bernardino suchitepéquez encontrados dos cadáveres identificados acribillados tiros torturados opunen eleve precio azucar grafica grupo trabajadores presentó ministerio economía solicitar acceda aumento precio azúcar gestionando propietarios varios ingenios foto julio h. garcía dirigentes federación autónoma sindical guatemala —fasgua— federación trabajadores unidos industria azúcar —fetulia— presentaron mañana ministerio economía solicitar autoridades ramo acceda `` aumento precio azúcar vienen '' gestionando dueños ingenios señores damian gomez secretario organización federación autónoma sindical mario rené santizo espino secretario general trabajadores industria azúcar señalaron inconveniencia aplique aumento precio azúcar consumidores acuerdo expresado dichos dirigentes empresarios azucareros debieron prever problemas precios afrontando exterior pasado disponer fondo estabilizador obtuvieron jugosas ganancias vista buenos precios azúcar mercado mundial opinión informantes injusto ahora recios azúcar comercio internacional andan mal ministerio economía acceda castigar consumidores nacionales recargo precio parte declarantes denunciaron haciendo efectivo aumento precio producto consumo popular además distribuyendo azúcar varias calidades perjuicio consumidores problemas —señalaron— vamos plantearlos autoridades ministerio economía intervenga inmediato\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Viewing a preprocessed document\n",
    "\n",
    "documents_filtered[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vectorizing preprocessed essays\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(documents_filtered) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['citadino',\n",
       " 'citado',\n",
       " 'citados',\n",
       " 'citar',\n",
       " 'ciudad',\n",
       " 'ciudadano',\n",
       " 'ciudadanos',\n",
       " 'ciudadanía',\n",
       " 'civil',\n",
       " 'civiles',\n",
       " 'clama',\n",
       " 'claman',\n",
       " 'clandestina',\n",
       " 'clandestinas',\n",
       " 'clandestino',\n",
       " 'clarificado',\n",
       " 'clarifique',\n",
       " 'claro',\n",
       " 'clase',\n",
       " 'clases']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Creating a vocabulary list corresponding to the vectors we created above\n",
    "\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "\n",
    "vocabulary[1140:1160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing an LDA model: 10 topics and 1500 iterations\n",
    "\n",
    "import lda\n",
    "\n",
    "model = lda.LDA(n_topics=10, n_iter=1500, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 100\n",
      "INFO:lda:vocab_size: 5711\n",
      "INFO:lda:n_words: 22174\n",
      "INFO:lda:n_topics: 10\n",
      "INFO:lda:n_iter: 1500\n",
      "/Users/anaconda3/envs/jupyter_usage/lib/python3.6/site-packages/lda/utils.py:55: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if sparse and not np.issubdtype(doc_word.dtype, int):\n",
      "INFO:lda:<0> log likelihood: -264743\n",
      "INFO:lda:<10> log likelihood: -204742\n",
      "INFO:lda:<20> log likelihood: -201040\n",
      "INFO:lda:<30> log likelihood: -198871\n",
      "INFO:lda:<40> log likelihood: -197530\n",
      "INFO:lda:<50> log likelihood: -196882\n",
      "INFO:lda:<60> log likelihood: -196205\n",
      "INFO:lda:<70> log likelihood: -195863\n",
      "INFO:lda:<80> log likelihood: -195257\n",
      "INFO:lda:<90> log likelihood: -195037\n",
      "INFO:lda:<100> log likelihood: -194634\n",
      "INFO:lda:<110> log likelihood: -194547\n",
      "INFO:lda:<120> log likelihood: -194444\n",
      "INFO:lda:<130> log likelihood: -194068\n",
      "INFO:lda:<140> log likelihood: -193933\n",
      "INFO:lda:<150> log likelihood: -193910\n",
      "INFO:lda:<160> log likelihood: -193912\n",
      "INFO:lda:<170> log likelihood: -193952\n",
      "INFO:lda:<180> log likelihood: -193653\n",
      "INFO:lda:<190> log likelihood: -193713\n",
      "INFO:lda:<200> log likelihood: -193534\n",
      "INFO:lda:<210> log likelihood: -193295\n",
      "INFO:lda:<220> log likelihood: -193446\n",
      "INFO:lda:<230> log likelihood: -193446\n",
      "INFO:lda:<240> log likelihood: -193347\n",
      "INFO:lda:<250> log likelihood: -193248\n",
      "INFO:lda:<260> log likelihood: -193251\n",
      "INFO:lda:<270> log likelihood: -192679\n",
      "INFO:lda:<280> log likelihood: -192964\n",
      "INFO:lda:<290> log likelihood: -192838\n",
      "INFO:lda:<300> log likelihood: -192913\n",
      "INFO:lda:<310> log likelihood: -192615\n",
      "INFO:lda:<320> log likelihood: -192932\n",
      "INFO:lda:<330> log likelihood: -192576\n",
      "INFO:lda:<340> log likelihood: -192511\n",
      "INFO:lda:<350> log likelihood: -192217\n",
      "INFO:lda:<360> log likelihood: -192181\n",
      "INFO:lda:<370> log likelihood: -192281\n",
      "INFO:lda:<380> log likelihood: -192397\n",
      "INFO:lda:<390> log likelihood: -192173\n",
      "INFO:lda:<400> log likelihood: -192272\n",
      "INFO:lda:<410> log likelihood: -192311\n",
      "INFO:lda:<420> log likelihood: -192234\n",
      "INFO:lda:<430> log likelihood: -192372\n",
      "INFO:lda:<440> log likelihood: -192099\n",
      "INFO:lda:<450> log likelihood: -192037\n",
      "INFO:lda:<460> log likelihood: -192092\n",
      "INFO:lda:<470> log likelihood: -191811\n",
      "INFO:lda:<480> log likelihood: -191898\n",
      "INFO:lda:<490> log likelihood: -191908\n",
      "INFO:lda:<500> log likelihood: -192145\n",
      "INFO:lda:<510> log likelihood: -191990\n",
      "INFO:lda:<520> log likelihood: -191899\n",
      "INFO:lda:<530> log likelihood: -192015\n",
      "INFO:lda:<540> log likelihood: -192049\n",
      "INFO:lda:<550> log likelihood: -191958\n",
      "INFO:lda:<560> log likelihood: -191763\n",
      "INFO:lda:<570> log likelihood: -191956\n",
      "INFO:lda:<580> log likelihood: -191893\n",
      "INFO:lda:<590> log likelihood: -192271\n",
      "INFO:lda:<600> log likelihood: -192158\n",
      "INFO:lda:<610> log likelihood: -192117\n",
      "INFO:lda:<620> log likelihood: -191819\n",
      "INFO:lda:<630> log likelihood: -192049\n",
      "INFO:lda:<640> log likelihood: -192331\n",
      "INFO:lda:<650> log likelihood: -192390\n",
      "INFO:lda:<660> log likelihood: -192264\n",
      "INFO:lda:<670> log likelihood: -192240\n",
      "INFO:lda:<680> log likelihood: -192206\n",
      "INFO:lda:<690> log likelihood: -192357\n",
      "INFO:lda:<700> log likelihood: -191966\n",
      "INFO:lda:<710> log likelihood: -192129\n",
      "INFO:lda:<720> log likelihood: -192118\n",
      "INFO:lda:<730> log likelihood: -192079\n",
      "INFO:lda:<740> log likelihood: -192149\n",
      "INFO:lda:<750> log likelihood: -192219\n",
      "INFO:lda:<760> log likelihood: -192035\n",
      "INFO:lda:<770> log likelihood: -192022\n",
      "INFO:lda:<780> log likelihood: -192294\n",
      "INFO:lda:<790> log likelihood: -192039\n",
      "INFO:lda:<800> log likelihood: -192167\n",
      "INFO:lda:<810> log likelihood: -192169\n",
      "INFO:lda:<820> log likelihood: -191866\n",
      "INFO:lda:<830> log likelihood: -192100\n",
      "INFO:lda:<840> log likelihood: -192166\n",
      "INFO:lda:<850> log likelihood: -191600\n",
      "INFO:lda:<860> log likelihood: -192062\n",
      "INFO:lda:<870> log likelihood: -192176\n",
      "INFO:lda:<880> log likelihood: -191976\n",
      "INFO:lda:<890> log likelihood: -192033\n",
      "INFO:lda:<900> log likelihood: -192048\n",
      "INFO:lda:<910> log likelihood: -191725\n",
      "INFO:lda:<920> log likelihood: -192015\n",
      "INFO:lda:<930> log likelihood: -191995\n",
      "INFO:lda:<940> log likelihood: -191946\n",
      "INFO:lda:<950> log likelihood: -191891\n",
      "INFO:lda:<960> log likelihood: -191437\n",
      "INFO:lda:<970> log likelihood: -191601\n",
      "INFO:lda:<980> log likelihood: -191697\n",
      "INFO:lda:<990> log likelihood: -191786\n",
      "INFO:lda:<1000> log likelihood: -191542\n",
      "INFO:lda:<1010> log likelihood: -191252\n",
      "INFO:lda:<1020> log likelihood: -191420\n",
      "INFO:lda:<1030> log likelihood: -191239\n",
      "INFO:lda:<1040> log likelihood: -191365\n",
      "INFO:lda:<1050> log likelihood: -191369\n",
      "INFO:lda:<1060> log likelihood: -191209\n",
      "INFO:lda:<1070> log likelihood: -191324\n",
      "INFO:lda:<1080> log likelihood: -191190\n",
      "INFO:lda:<1090> log likelihood: -191471\n",
      "INFO:lda:<1100> log likelihood: -191180\n",
      "INFO:lda:<1110> log likelihood: -191151\n",
      "INFO:lda:<1120> log likelihood: -191199\n",
      "INFO:lda:<1130> log likelihood: -191125\n",
      "INFO:lda:<1140> log likelihood: -191072\n",
      "INFO:lda:<1150> log likelihood: -191393\n",
      "INFO:lda:<1160> log likelihood: -191410\n",
      "INFO:lda:<1170> log likelihood: -191099\n",
      "INFO:lda:<1180> log likelihood: -191238\n",
      "INFO:lda:<1190> log likelihood: -191169\n",
      "INFO:lda:<1200> log likelihood: -191032\n",
      "INFO:lda:<1210> log likelihood: -191090\n",
      "INFO:lda:<1220> log likelihood: -191039\n",
      "INFO:lda:<1230> log likelihood: -191322\n",
      "INFO:lda:<1240> log likelihood: -190671\n",
      "INFO:lda:<1250> log likelihood: -190694\n",
      "INFO:lda:<1260> log likelihood: -190727\n",
      "INFO:lda:<1270> log likelihood: -191083\n",
      "INFO:lda:<1280> log likelihood: -190758\n",
      "INFO:lda:<1290> log likelihood: -190758\n",
      "INFO:lda:<1300> log likelihood: -191028\n",
      "INFO:lda:<1310> log likelihood: -190968\n",
      "INFO:lda:<1320> log likelihood: -190729\n",
      "INFO:lda:<1330> log likelihood: -191011\n",
      "INFO:lda:<1340> log likelihood: -190857\n",
      "INFO:lda:<1350> log likelihood: -191138\n",
      "INFO:lda:<1360> log likelihood: -191205\n",
      "INFO:lda:<1370> log likelihood: -190941\n",
      "INFO:lda:<1380> log likelihood: -190891\n",
      "INFO:lda:<1390> log likelihood: -191038\n",
      "INFO:lda:<1400> log likelihood: -191082\n",
      "INFO:lda:<1410> log likelihood: -190920\n",
      "INFO:lda:<1420> log likelihood: -190523\n",
      "INFO:lda:<1430> log likelihood: -190674\n",
      "INFO:lda:<1440> log likelihood: -190801\n",
      "INFO:lda:<1450> log likelihood: -190764\n",
      "INFO:lda:<1460> log likelihood: -190532\n",
      "INFO:lda:<1470> log likelihood: -190813\n",
      "INFO:lda:<1480> log likelihood: -190818\n",
      "INFO:lda:<1490> log likelihood: -190828\n",
      "INFO:lda:<1499> log likelihood: -190720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lda.lda.LDA at 0x118ece470>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Fitting the model using our list of vectors\n",
    "\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "pick up camión vehículo ruta zona secuestradores estudiante edad placas\n",
      "\n",
      "Topic 1:\n",
      "años edad sido cadáver autoridades escuintla balazos cadáveres víctimas san\n",
      "\n",
      "Topic 2:\n",
      "policía colonia nacional agentes estudiante carlos tres corado cuerpo policías\n",
      "\n",
      "Topic 3:\n",
      "policía dos señor desconocidos según varios ayer lugar muerte autoridades\n",
      "\n",
      "Topic 4:\n",
      "hombres aldea campesinos armados verde grupo vecinos olivo departamento miembros\n",
      "\n",
      "Topic 5:\n",
      "alcalde iztapa bar cruz ortega puerto luis gonzález hospital lópez\n",
      "\n",
      "Topic 6:\n",
      "atentado alcalde detectives herido cuerpo casa zona guerra hospital nacional\n",
      "\n",
      "Topic 7:\n",
      "vecinos presidente terreno escuela empresa problema sector instalaciones diciendo comité\n",
      "\n",
      "Topic 8:\n",
      "ser puede problemas señor cantidad casos viene hacer oficina agua\n",
      "\n",
      "Topic 9:\n",
      "pues zona hombre ejército llegó familia hace evitar gobierno militares\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Viewing the top 50 words in each 'topic'\n",
    "\n",
    "topic_word = model.topic_word_\n",
    "\n",
    "n_top_words = 10\n",
    "\n",
    "for i, topic_distribution in enumerate(topic_word):\n",
    "    topic_words = np.array(vocabulary)[np.argsort(topic_distribution)][:-(n_top_words+1):-1]\n",
    "    print('Topic ' + str(i) + ':')\n",
    "    print(' '.join(topic_words))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make stopwords in a single variable and make a class that \n",
    "#reprocesses the documents into a dictionary for gensim format with tokenization. \n",
    "import logging \n",
    "stops = '/Users/jgo384/nltk_data/corpora/stopwords/spanish.txt'\n",
    "\n",
    "stoplist = open(stops).read()\n",
    "stoplist = stoplist.split('\\n')\n",
    "class MyCorpus(object):\n",
    "\n",
    "    def __init__(self, topdir, stoplist):\n",
    "        self.topdir = topdir\n",
    "        self.stoplist = stoplist\n",
    "        self.dictionary = gensim.corpora.Dictionary(iter_docs(topdir, stoplist))\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for tokens in iter_docs(self.topdir, self.stoplist):\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', \n",
    "                    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate function to iterate through the documents separately to tokenize and preprocess with lower and accent remove\n",
    "def iter_docs(topdir, stoplist):\n",
    "    for fn in os.listdir(topdir):\n",
    "        fin = open(os.path.join(topdir, fn), 'rb')\n",
    "        text = fin.read()\n",
    "        fin.close()\n",
    "        yield (x for x in \n",
    "            gensim.utils.tokenize(text, lowercase=True, deacc=True, \n",
    "                                  errors=\"ignore\")\n",
    "            if x not in stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gensim doesn't like the texts directory to be two separate so they were combined\n",
    "try: os.mkdir('/sharedfolder/models/dir')\n",
    "except: pass\n",
    "TEXTS_DIR = \"/sharedfolder/tempdirtesting\"\n",
    "MODELS_DIR = \"/sharedfolder/models/dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the fist is a bit of overkill since it reapplies stopwords, it also generates tokenization again\n",
    "#the issue is that I can't bring the other variables over because it dislikes nltk's tokenization for utf-8\n",
    "#the second creates a dictionary which is just a id per word list\n",
    "#the third creates pairings between document, id, and word with a frequency count in a dense matrix\n",
    "corpus = MyCorpus(TEXTS_DIR, stoplist)\n",
    "corpus.dictionary.save(os.path.join(MODELS_DIR, 'spaport.dict'))\n",
    "gensim.corpora.MmCorpus.serialize(os.path.join(MODELS_DIR, \"spaport.mm\"), \n",
    "                                  corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logging just to keep track of time for these, much slower than Mallet or lda\n",
    "#dictionary gets loaded from dict file\n",
    "#corpus is loaded from path and linked to matrix of values for frequency, id and doc\n",
    "#50 topics seems to work best and keeping the passes(iterations) at the same number as Mallet (painfully slow)\n",
    "numtopics = 50\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', \n",
    "                    level=logging.INFO)\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary.load(os.path.join(MODELS_DIR, \n",
    "                                            \"spaport.dict\"))\n",
    "corpus = gensim.corpora.MmCorpus(os.path.join(MODELS_DIR, \"spaport.mm\"))\n",
    "\n",
    "# Project to LDA space\n",
    "polylingualmodel = gensim.models.LdaMulticore(corpus, id2word=dictionary, num_topics=numtopics, \n",
    "                                              workers=8, random_state=1, passes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the model for persistency\n",
    "polylingualmodel.save('polylingualmodel2.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a bag-of-words from the corpus with doc id, word, frequency count array\n",
    "bow_corpus = MmCorpus(\"/sharedfolder/models/dir/spaport.mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This shows a random sorting of topics with the weight across the entire corpus \n",
    "#\"topic distribution for the given document bow, as a list of (topic_id, topic_probability) 2-tuples\"\n",
    "get_documents_topics = polylingualmodel.get_document_topics(bow_corpus, \n",
    "                                                            minimum_probability=.001, \n",
    "                                                            minimum_phi_value=0.01, \n",
    "                                                            per_word_topics=True)\n",
    "get_documents_topics[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This prints a topic at a time and is useful for inspecting topics one by one. The one listed here\n",
    "#bears some slight resemblance to 21 in Mallet\n",
    "print_topics = polylingualmodel.print_topics(num_topics=1, num_words=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From gensim site \"tuples with (topic_repr, coherence_score), where topic_repr is a list of \n",
    "#representations of the topn terms for the topic. \n",
    "#The terms are represented as tuples of (membership_in_topic, token). \n",
    "#The coherence_score is a float\".\n",
    "top_topics = polylingualmodel.top_topics(corpus=corpus, texts=None, dictionary='spaport.dict', \n",
    "                                         window_size=None, coherence='u_mass', \n",
    "                                         topn=50, processes=-1)\n",
    "top_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#even with pprint this is quite difficult to sort through but there seems to be an issue with the weights\n",
    "#all being too low at .00*\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/sharedfolder/models/dir')\n",
    "d = gensim.corpora.Dictionary.load('spaport.dict')\n",
    "c = gensim.corpora.MmCorpus('spaport.mm')\n",
    "vizmod = gensim.models.LdaMulticore.load('polylingualmodel2.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "data = pyLDAvis.gensim.prepare(vizmod, c, d)\n",
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
